# create_dataset.py


np.random.normal()	                  Simulate realistic distributions (like normal user traffic)
np.random.uniform()	                  Simulate random ranges (like error rates)
pd.DataFrame()	                      Create table-like structure
.concat() + .sample()	              Merge and shuffle datasets
to_csv()	                          Save your data to a 
pd.concat()                           joins the normal and attack data together
.sample(frac=1)                       shuffles all rows randomly
.reset_index(drop=True)               resets the row numbers after shuffling

#app/main.py

r.pipeline()

This creates a "pipeline" object. A pipeline batches multiple Redis commands together and sends them to the Redis server in a single round trip.
# Machine Learning Model used

"Why IsolationForest?"

Reason	                                          Explanation

Built for Anomalies	                              Designed specifically to find outliers
Fast and Efficient	                              Scales to large datasets
No Labels Needed	                              Works in unsupervised settings
Real-World Ready	                              Works great for security, fraud, API monitoring


CI/CD with GitHub Actions

What is CI/CD?

Continuous Integration (CI): Automatically testing your code every time you push a change to GitHub. 
This catches bugs early and ensures your project is always in a working state.

Continuous Deployment (CD): Automatically deploying your application after the tests pass. 
For this project, our "deployment" step will be to build the Docker image to prove it works.


Tier 1, Enhancement 1: Add API Key Authentication ✅
Tier 1, Enhancement 2: More Sophisticated Rate Limiting ✅
Tier 1, Enhancement 3: Allow Whitelisting/Blacklisting of IPs ✅

Tier 2, Enhancement 1: More Advanced AI Model Features (Feature Engineering)

1. What is Feature Engineering for AI Models?

Feature engineering is the process of using domain knowledge to extract features from raw data 
that are then used to train machine learning models. The goal is to create features that make the 
machine learning algorithm work better.

User-Agent Analysis:

* is_empty_user_agent (Binary: 0 or 1): Is the User-Agent header missing or empty? (Common for simple bots).

* user_agent_length (Numerical): The length of the User-Agent string. (Very short or very long UAs can be suspicious).

Request Body Analysis (for POST/PUT/PATCH requests):

* request_body_size (Numerical): The size of the request body in bytes. (Sudden large payloads can indicate data exfiltration or malformed requests).

* is_json_content_type (Binary: 0 or 1): Is the Content-Type header application/json? (Many APIs expect JSON, but non-JSON or unexpected types could be suspicious).

Header Analysis:

* num_headers (Numerical): The total number of headers in the request. (Unusually high or low numbers could be suspicious).


 if request.method in ["POST", "PUT", "PATCH"]:
        # Await the body to get its size. This consumes the body, so we'll need to re-read it later.
        # For proxying, we need to ensure the body is available again.
        # FastAPI's Request.body() can only be awaited once.
        # To handle this, we'll read it once, store it, and then pass it to requests.request.
        body_bytes = await request.body()
        request_body_size = len(body_bytes)
        request._body = body_bytes # Store it back for the proxy request

Tier 2, Enhancement 2: Persistent Dashboard History ✅

Persistent dashboard history means storing your key metrics (total requests, blocked requests, etc.) in a more 
durable and query-friendly database over time. This allows you to:

* Review Past Performance: See trends, identify peak attack times, or analyze long-term traffic patterns.
* Post-Mortem Analysis: Investigate incidents that happened days or weeks ago.
* Reporting: Generate reports on API usage and security over extended periods.

How it Works (Conceptually):
       
       1. Background Process: A new, lightweight background process will periodically wake up.
       2. Fetch Live Data: This process will query PathHelm's /pathhelm/status endpoint (using the admin key).
       3. Store Historically: It will then take these live metrics and append them to a simple, 
           persistent database. For simplicity and to avoid introducing a new complex database, we can use a SQLite database. SQLite is a 
           file-based database, meaning it's very easy to set up (just a file!) and doesn't require a separate server process.
       4. Dashboard Update: The Streamlit dashboard will then be modified to query this SQLite database for historical data, allowing users to select date ranges or view trends.

Tier 2, Enhancement 3: Centralized Logging

Centralized logging involves collecting logs from all your services into a single, searchable, and analyzable platform

Purpose:

    1. Troubleshooting: Quickly diagnose issues by searching across all logs.
    2. Monitoring & Alerting: Set up alerts for specific error patterns or security events.
    3. Auditing: Keep a record of all requests and actions for compliance or security audits.
    4. Performance Analysis: Analyze request patterns, latencies, and error rates over time.

How it Works (Conceptually):

    1. Structured Logging: Instead of just plain text, logs are formatted as JSON. This makes them machine-readable and easy to parse by a logging system.
    2. Log Shipper: A lightweight agent (like Filebeat or Fluentd) runs in each service's container (or as a sidecar) or your Docker logging driver is configured to send logs directly. This agent collects logs and forwards them to a central logging server.
    3. Logging Server: A central server (e.g., Elasticsearch or Loki) stores and indexes the logs.
    4. Visualization/Analysis: A UI (e.g., Kibana for Elasticsearch, Grafana for Loki) allows you to search, filter, visualize, and analyze your logs.